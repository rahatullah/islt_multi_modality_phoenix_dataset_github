{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d162ada",
   "metadata": {},
   "source": [
    "# **Import all required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa8d849-bde6-4b5b-9511-583d502d89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tensorflow.keras.applications import EfficientNetB7\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from openpyxl import load_workbook\n",
    "import portalocker\n",
    "import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713012a",
   "metadata": {},
   "source": [
    "# **Initialize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb543414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    base_model = EfficientNetB7(weights='imagenet', include_top=False)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "    return feature_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acc02c5",
   "metadata": {},
   "source": [
    "# **Function to save checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac97e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, checkpoint_name, list_of_inputs):\n",
    "    unstable_path = os.path.join(checkpoint_path, \"unstable\")\n",
    "    unstable_file = os.path.join(unstable_path, checkpoint_name)\n",
    "    if not os.path.exists(unstable_path):\n",
    "        os.makedirs(unstable_path)\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        with gzip.GzipFile(fileobj=tmp_file, mode='wb') as gz:\n",
    "            pickle.dump(list_of_inputs, gz)\n",
    "        tmp_filename = tmp_file.name\n",
    "\n",
    "    try:\n",
    "        shutil.move(tmp_filename, unstable_file)\n",
    "        shutil.move(unstable_file, os.path.join(checkpoint_path, checkpoint_name))\n",
    "        print(\"Backup made at: \" + str(checkpoint_path))\n",
    "\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        backup_folder = os.path.join(checkpoint_path, \"backup\")\n",
    "        if not os.path.exists(backup_folder):\n",
    "            os.makedirs(backup_folder)\n",
    "        backup_unstable_folder = os.path.join(backup_folder, f\"unstable_{checkpoint_name}_{timestamp}\")\n",
    "        shutil.copytree(unstable_path, backup_unstable_folder)\n",
    "        print(\"Unstable folder backed up at: \" + str(backup_unstable_folder))\n",
    "    except Exception as e:\n",
    "        print(\"\\n\\nError!!!! Could not create backup: \" + str(e))\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02031a54",
   "metadata": {},
   "source": [
    "# **Function to load checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, checkpoint_name):\n",
    "    backup_file = os.path.join(checkpoint_path, checkpoint_name)\n",
    "    if os.path.exists(backup_file):\n",
    "        print(\"Loading from: \" + str(backup_file))\n",
    "        print(\"\\n*************************\\n*************************\\n*************************\\n*** Checkpoint Loaded ***\\n*************************\\n*************************\\n*************************\\n\")\n",
    "        try:\n",
    "            with open(backup_file, 'rb') as f:\n",
    "                portalocker.lock(f, portalocker.LOCK_SH)\n",
    "                try:\n",
    "                    with gzip.GzipFile(fileobj=f) as gz:\n",
    "                        data = pickle.load(gz)\n",
    "                finally:\n",
    "                    portalocker.unlock(f)\n",
    "                    print(\"File unlocked successfully\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read and lock the file: {e}\")\n",
    "            raise e\n",
    "    else:\n",
    "        print(\"Creating at: \" + str(backup_file))\n",
    "        print(\"\\n****************************************\\n****************************************\\n****************************************\\n*** Checkpoint Loading Failed!!!!!!! ***\\n****************************************\\n****************************************\\n****************************************\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbdd289",
   "metadata": {},
   "source": [
    "# **Function to get features for a single video**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317cddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(filename, destination, feature_extractor):\n",
    "    input_string = filename\n",
    "    pattern = r'\\d+'\n",
    "    match = re.search(pattern, input_string)\n",
    "    if match:\n",
    "        first_match = match.group()\n",
    "        input_folder = os.path.join(os.path.dirname(os.path.abspath(__file__)), destination, first_match, input_string)\n",
    "        try:\n",
    "            file_paths_frames = [file for file in sorted(os.listdir(input_folder)) if file.endswith(\".jpg\")]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "        features_listofList = []\n",
    "        for indx, frame_file in enumerate(file_paths_frames):\n",
    "            frame_filename = os.path.join(input_folder, frame_file)\n",
    "            image = cv2.imread(frame_filename)\n",
    "            \n",
    "            # Resize image while maintaining aspect ratio and padding to 600x600\n",
    "            h, w, _ = image.shape\n",
    "            target_size = 600\n",
    "            scale = min(target_size / h, target_size / w)\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            resized_image = cv2.resize(image, (new_w, new_h))\n",
    "            top_pad = (target_size - new_h) // 2\n",
    "            bottom_pad = target_size - new_h - top_pad\n",
    "            left_pad = (target_size - new_w) // 2\n",
    "            right_pad = target_size - new_w - left_pad\n",
    "            image = cv2.copyMakeBorder(resized_image, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "            \n",
    "            image = preprocess_input(image)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            spatial_embedding = feature_extractor.predict(image)[0]\n",
    "            features_listofList.append(spatial_embedding)\n",
    "        return torch.tensor(features_listofList)\n",
    "    else:\n",
    "        print(\"No match found for: \" + input_string + \"\\n\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a2701",
   "metadata": {},
   "source": [
    "# **Function to check for inconsistent data and pad non-square data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a692ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_pad_data(data):\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    max_length = max(item['sign'].shape[0] for item in data)\n",
    "    expected_shape = data[0]['sign'].shape[1:]\n",
    "    \n",
    "    for item in data:\n",
    "        tensor_shape = item['sign'].shape\n",
    "        if tensor_shape[0] < max_length:\n",
    "            padding = torch.zeros((max_length - tensor_shape[0], *expected_shape))\n",
    "            item['sign'] = torch.cat((item['sign'], padding), dim=0)\n",
    "        elif tensor_shape[0] > max_length:\n",
    "            item['sign'] = item['sign'][:max_length]\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d0a34",
   "metadata": {},
   "source": [
    "# **Function to create pickle for an excel file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4cfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pickle(config, frame_dest, checkpoint_path):\n",
    "    vw_dest = config['vw_dest']\n",
    "    vo_dest = config['vo_dest']\n",
    "    checkpoint_name = config['checkpoint_name']\n",
    "\n",
    "    feature_extractor = initialize_model()\n",
    "\n",
    "    workbook = load_workbook(os.path.join(os.path.dirname(os.path.abspath(__file__)), vw_dest))\n",
    "    sheet = workbook.active\n",
    "    excel_data = []\n",
    "    for row in sheet.iter_rows(values_only=True):\n",
    "        excel_data.append(row)\n",
    "\n",
    "    list_of_inputs = load_checkpoint(checkpoint_path, checkpoint_name)\n",
    "    if list_of_inputs is None:\n",
    "        list_of_inputs = []\n",
    "\n",
    "    checkpoint_range = 20\n",
    "    none_counter = 0\n",
    "    flag = 0\n",
    "    for index in range(len(list_of_inputs), len(excel_data), checkpoint_range):\n",
    "        if flag == 1:\n",
    "            exit()\n",
    "        batch_list_of_inputs = []\n",
    "        for tmp in excel_data[index:index + checkpoint_range]:\n",
    "            features = get_features(str(tmp[0]), frame_dest, feature_extractor)\n",
    "            if features is not None:\n",
    "                none_counter = 0\n",
    "                if len(features) > 0:\n",
    "                    data_dict = {\n",
    "                        'name': tmp[0],\n",
    "                        'signer': tmp[1],\n",
    "                        'gloss': tmp[2],\n",
    "                        'text': tmp[3],\n",
    "                        'sign': features + 1e-8\n",
    "                    }\n",
    "                    batch_list_of_inputs.append(data_dict)\n",
    "            else:\n",
    "                none_counter += 1\n",
    "                if none_counter >= checkpoint_range - 1:\n",
    "                    flag = 1\n",
    "                    break\n",
    "        if flag == 1:\n",
    "            break\n",
    "        \n",
    "        list_of_inputs.extend(batch_list_of_inputs)\n",
    "\n",
    "        list_of_inputs = validate_and_pad_data(list_of_inputs)\n",
    "        \n",
    "        save_checkpoint(checkpoint_path, checkpoint_name, list_of_inputs)\n",
    "        torch.cuda.empty_cache()\n",
    "        list_of_inputs = load_checkpoint(checkpoint_path, checkpoint_name)\n",
    "\n",
    "    with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), vo_dest), 'wb') as f:\n",
    "        with gzip.GzipFile(fileobj=f, mode='wb') as gz:\n",
    "            pickle.dump(list_of_inputs, gz)\n",
    "\n",
    "    print(f\"Done processing {vw_dest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d3ff3",
   "metadata": {},
   "source": [
    "# **The function upon calling executes pickle_create for all the excel files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51efc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_pickle_creations(configurations, frame_dest, checkpoint_path):\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for config in configurations:\n",
    "            print(f\"Submitting {config['vw_dest']} -> {config['vo_dest']} with checkpoint {config['checkpoint_name']}\")\n",
    "            futures.append(executor.submit(create_pickle, config, frame_dest, checkpoint_path))\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98218e94",
   "metadata": {},
   "source": [
    "# **The function to combine pickles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dab0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pickles(input_paths, output_path):\n",
    "    combined_data = []\n",
    "\n",
    "    for path in input_paths:\n",
    "        with open(os.path.join(os.path.dirname(os.path.abspath(__file__)), path), 'rb') as f:\n",
    "            with gzip.GzipFile(fileobj=f) as gz:\n",
    "                data = pickle.load(gz)\n",
    "                combined_data.extend(data)\n",
    "                print(f\"Loaded {len(data)} items from {path}\")\n",
    "\n",
    "    with open(os.path.join(os.path.dirname(os.path.abspath(__file__)),output_path), 'wb') as f:\n",
    "        with gzip.GzipFile(fileobj=f, mode='wb') as gz:\n",
    "            pickle.dump(combined_data, gz)\n",
    "            print(f\"Combined data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535fd20",
   "metadata": {},
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.freeze_support()\n",
    "\n",
    "    # List of configurations to process\n",
    "    configurations = [\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Validation.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data.dev\",\n",
    "            'checkpoint_name': 'dev_checkpoint.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Test.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data.test\",\n",
    "            'checkpoint_name': 'test_checkpoint.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_0.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data0.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_0.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_1.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data1.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_1.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_2.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data2.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_2.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_3.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data3.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_3.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_4.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data4.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_4.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_5.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data5.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_5.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_6.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data6.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_6.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_7.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data7.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_7.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_8.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data8.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_8.pkl'\n",
    "        },\n",
    "        {\n",
    "            'excel_dest': \"Dataset/excels/Train/Train_9.xlsx\",\n",
    "            'output_dest': \"Dataset/Pickles/excel_data9.train\",\n",
    "            'checkpoint_name': 'train_checkpoint_9.pkl'\n",
    "        }\n",
    "    ]\n",
    "    frame_dest = \"Dataset/Final folder for frames\"\n",
    "    store_to_path = 'C:\\\\Users\\\\Admin\\\\Rahul\\\\islt_multi_modality_phoenix_dataset\\\\Pre-Processing\\\\Pickle maker\\\\Dataset\\\\Checkpoint\\\\'\n",
    "    run_multiple_pickle_creations(configurations, frame_dest, store_to_path)\n",
    "    print(\"Done creating pickle files for all configurations.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114aa6d",
   "metadata": {},
   "source": [
    "# **Code to merge the train pickles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f56bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of input pickle files\n",
    "if __name__ == \"__main__\":\n",
    "    input_paths = [\n",
    "        \"Dataset/Pickles/excel_data0.train\",\n",
    "        \"Dataset/Pickles/excel_data1.train\",\n",
    "        \"Dataset/Pickles/excel_data2.train\",\n",
    "        \"Dataset/Pickles/excel_data3.train\",\n",
    "        \"Dataset/Pickles/excel_data4.train\",\n",
    "        \"Dataset/Pickles/excel_data5.train\",\n",
    "        \"Dataset/Pickles/excel_data6.train\",\n",
    "        \"Dataset/Pickles/excel_data7.train\",\n",
    "        \"Dataset/Pickles/excel_data8.train\",\n",
    "        \"Dataset/Pickles/excel_data9.train\",\n",
    "        # Add more paths as needed\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Output path for the combined pickle file\n",
    "    output_path = \"Dataset/Pickles/excel_data.train\"\n",
    "\n",
    "    combine_pickles(input_paths, output_path)\n",
    "    print(\"The fragmented train pickle has been merged\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
