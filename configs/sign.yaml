name: sign_experiment_multistream
data:
    data_path: ./data/ # Path to the dataset (adjust if necessary)
    version: phoenix_2014_trans
    sgn: sign
    txt: text
    gls: gloss
    train: excel_data.train
    dev: excel_data.dev
    test: excel_data.test
    feature_size: 2560
    level: word
    txt_lowercase: true
    max_sent_length: 500
    random_train_subset: -1
    random_dev_subset: -1

# Add paths for the multi-stream inputs (optional, if preprocessing into separate folders)
streams:
    visual_stream: ./data/visual/
    emotion_stream: ./data/emotion/
    gesture_stream: ./data/gesture/

testing:
    recognition_beam_sizes:
    - 1
    - 2
    - 3
    translation_beam_sizes:
    - 1
    - 2
    - 3
    translation_beam_alphas:
    - -1
    - 0
    - 1

training:
    reset_best_ckpt: false
    reset_scheduler: false
    reset_optimizer: false
    random_seed: 42
    model_dir: "./sign_sample_model/"
    recognition_loss_weight: 1.0
    translation_loss_weight: 1.0
    eval_metric: bleu
    optimizer: adam
    learning_rate: 0.0001
    batch_size: 8
    num_valid_log: 5
    epochs: 50000
    early_stopping_metric: eval_metric
    use_cuda: true  # Enable CUDA for training on GPUs
    translation_max_output_length: 40

model:
    initializer: xavier
    encoder:
        type: transformer
        num_layers: 3
        num_heads: 32
        embeddings:
            embedding_dim: 512
            dropout: 0.1
        hidden_size: 512
        ff_size: 2048
        dropout: 0.1

    # Adjustments for multi-stream encoder settings
    encoders:
        visual_encoder:
            type: transformer
            num_layers: 3
            num_heads: 32
            hidden_size: 512
            ff_size: 2048
        emotion_encoder:
            type: transformer
            num_layers: 3
            num_heads: 32
            hidden_size: 512
            ff_size: 2048
        gesture_encoder:
            type: transformer
            num_layers: 3
            num_heads: 32
            hidden_size: 512
            ff_size: 2048

    decoder:
        type: transformer
        num_layers: 3
        num_heads: 32
        hidden_size: 512
        ff_size: 2048
